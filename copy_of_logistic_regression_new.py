# -*- coding: utf-8 -*-
"""Copy of Logistic_Regression_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jzFeJPhtzUzpb1AqYtS8LdkPcFmD2Oca

# **LOGISTIC REGRESSION**

1. Data Exploration:  
a. Load the dataset and perform exploratory data analysis (EDA).  
b. Examine the features, their types, and summary statistics.  
c. Create visualizations such as histograms, box plots, or pair plots to visualize the distributions and relationships between features.  
Analyze any patterns or correlations observed in the data.  
2. Data Preprocessing:  
a. Handle missing values (e.g., imputation).   
b. Encode categorical variables.  
3. Model Building:  
a. Build a logistic regression model using appropriate libraries (e.g., scikit-learn).  
b. Train the model using the training data.
4. Model Evaluation:  
a. Evaluate the performance of the model on the testing data using accuracy, precision, recall, F1-score, and ROC-AUC score.  
Visualize the ROC curve.
5. Interpretation:  
a. Interpret the coefficients of the logistic regression model.  
b. Discuss the significance of features in predicting the target variable (survival probability in this case).
6. Deployment with Streamlit:  
In this task, you will deploy your logistic regression model using Streamlit. The deployment can be done locally or online via Streamlit Share. Your task includes creating a Streamlit app in Python that involves loading your trained model and setting up user inputs for predictions.

(optional)For online deployment, use Streamlit Community Cloud, which supports deployment from GitHub repositories.
Detailed deployment instructions are available in the Streamlit Documentation.
https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')

"""# **1. Data Exploration:**"""

train_df=pd.read_csv('Titanic_train.csv')
train_df

test_df=pd.read_csv('Titanic_test.csv')
test_df

train_df.head()

test_df.head()

train_df.tail()

test_df.tail()

train_df.info()

test_df.info()

train_df.shape

test_df.shape

train_df.dtypes

test_df.dtypes

train_df.describe()

test_df.describe()

train_df.isnull().sum()

#we found there are null values in 'Age','Cabin','Embarked'

test_df.isnull().sum()

#we  found there are null values in 'Age','Fare','Cabin'

#HISTOGRAM for Train data
# Age Distribution
import plotly.express as px
fig = px.histogram(train_df, x='Age', nbins=20,title='Age Distribution in Training Dataset')
fig.update_layout(xaxis_title='Age', yaxis_title='Frequency')
fig.show()

# Fare Distribution
fig = px.histogram(train_df, x='Fare', nbins=20, title='Fare Distribution in Training Dataset')
fig.update_layout(xaxis_title='Fare', yaxis_title='Frequency')
fig.show()

"""In Train data

Most of the members falls under age of 17.5 - 27.5

Fare of 0-25 has high no of members with 557 count
"""

#HISTOGRAM for Test data
#Age Distribution
import plotly.express as px
fig = px.histogram(test_df, x='Age', nbins=20,title='Age Distribution in Testing Dataset')
fig.update_layout(xaxis_title='Age', yaxis_title='Frequency')
fig.show()

# Fare Distribution
fig = px.histogram(test_df, x='Fare', nbins=20, title='Fare Distribution in Testing Dataset')
fig.update_layout(xaxis_title='Fare', yaxis_title='Frequency')
fig.show()

"""In Test data

Most of the members of age between 17.5 to 32.5 are more in number

Fare between 0-25 are having more members
"""

#BARPLOT for Train data
fig = px.bar(train_df['Sex'].value_counts(),
             x=train_df['Sex'].value_counts().index,
             y=train_df['Sex'].value_counts().values,
             labels={'x':'Sex', 'y':'Count'},
             title='Count of Passengers by Sex in Training Dataset')
fig.show()

# Count of Passengers by Embarked Port
fig = px.bar(train_df['Embarked'].value_counts(),
             x=train_df['Embarked'].value_counts().index,
             y=train_df['Embarked'].value_counts().values,
             labels={'x':'Embarked Port', 'y':'Count'},
             title='Count of Passengers by Embarked Port in Training Dataset')
fig.show()

"""In Train data

Embarked - Port of Embarkation S - Southampton has more passengers

Male are more in members in boat
"""

#BARPLOT for Test data
fig = px.bar(test_df['Sex'].value_counts(),
             x=test_df['Sex'].value_counts().index,
             y=test_df['Sex'].value_counts().values,
             labels={'x':'Sex', 'y':'Count'},
             title='Count of Passengers by Sex in Testing Dataset')
fig.show()

# Count of Passengers by Embarked Port
fig = px.bar(test_df['Embarked'].value_counts(),
             x=test_df['Embarked'].value_counts().index,
             y=test_df['Embarked'].value_counts().values,
             labels={'x':'Embarked Port', 'y':'Count'},
             title='Count of Passengers by Embarked Port in Testing Dataset')
fig.show()

"""In Test data

Embarked - Port of Embarkation S - Southampton has more passengers

Male are more in members in boat
"""

#BOXPLOT for Fare column in Train data
class_fare = train_df.pivot_table(index='Pclass', values='Fare',aggfunc=np.sum)
class_fare.plot(kind='bar')
plt.xlabel('Pclass')
plt.ylabel('Total Fare')
plt.xticks(rotation=0)
plt.show()

#BOXPLOT for Fare column in Test data
class_fare = test_df.pivot_table(index='Pclass', values='Fare',aggfunc=np.sum)
class_fare.plot(kind='bar')
plt.xlabel('Pclass')
plt.ylabel('Total Fare')
plt.xticks(rotation=0)
plt.show()

#Class-1 has more fare compared to other class in both data's

#Pairplot for train data
sns.pairplot(train_df)
plt.show()

#Pairplot for test data
sns.pairplot(test_df)
plt.show()

#Boxplot for pclass and fare for train data

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
sns.boxplot(x='Pclass', y='Fare', data=train_df)
plt.title('Boxplot of Fare by Passenger Class')
plt.xlabel('Passenger Class')
plt.ylabel('Fare')
plt.show()

#Boxplot for pclass and fare for test data
sns.boxplot(x='Pclass', y='Fare', data=test_df)
plt.title('Boxplot of Fare by Passenger Class')
plt.xlabel('Passenger Class')
plt.ylabel('Fare')
plt.show()

"""Analyze any patterns or correlations observed in the data.

Pclass and Fare: There's a negative correlation between Pclass (Passenger Class) and Fare. This means higher passenger classes tend to have higher fares, which is expected as first-class tickets are more expensive. You visualized this using a bar plot.

Pclass and Survival: Pclass is likely correlated with survival. We need to explore this further using visualizations or statistical tests. Lower Pclass (1st class) might indicate higher survival rates.

Age Distribution: The age distribution is slightly skewed, with a higher concentration of passengers in the 20-30 age range. You visualized this using histograms.

Fare Distribution: The fare distribution is heavily skewed to the right, with most fares concentrated in the lower range. You visualized this using histograms.

Sex: There were more male passengers than female passengers. This is evident from the bar plots you created.

Embarked Port: Most passengers embarked from Southampton (Embarked = S). This is evident from the bar plots you created.

# **2. Data Preprocessing:**
"""

#For Data Preprocessing we concat both the dataframes and preprocess the data

# combine two dataframes
data = pd.concat([train_df, test_df], axis=0)
data = data.reset_index(drop=True)
data.head()

data.info()

data.isnull().sum()

"""a. Handle missing values (e.g., imputation)."""

#Drop Cabin column because of high null values
data=data.drop(['Cabin'],axis=1)

data

#For Age column calculate mean
data['Age'].mean()

#For Fare column calculate mean
data['Fare'].mean()

# fill missing values using mean of the numerical column
data['Age'] = data['Age'].fillna(data['Age'].mean())
data['Fare'] = data['Fare'].fillna(data['Fare'].mean())

# fill missing values using mode of the categorical column
data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])

data.info()

"""## **Log transformation for uniform data distribution**"""

#For Example
sns.distplot(data['Fare'])
plt.show()

data['Fare'] = np.log(data['Fare']+1)
data['Fare']

sns.distplot(data['Fare'])
plt.show()

#Calculating correlation matrix for numerical features
numerical_features = data.select_dtypes(include=['number']) #Select Numerical columns
corr = numerical_features.corr()
plt.figure(figsize=(15, 9))
sns.heatmap(corr, annot=True, cmap='coolwarm')

data.head()

# drop unnecessary columns
df= data.drop(columns=['Name', 'Ticket'], axis=1)
df.head()

df

"""b.Encode categorical variables."""

#Use label encoder

from sklearn.preprocessing import LabelEncoder
cols = ['Sex','Embarked']
le = LabelEncoder()

for col in cols:
    df[col] = le.fit_transform(df[col])
df.head()

df['Embarked'].value_counts()

"""# **3. Model Building:**

## a. Build a logistic regression model using appropriate libraries (e.g., scikit-learn).
"""

#Logistic regression is a statistical technique used to predict the probability of an outcome or a categorical response variable based on one or more predictor variables

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report

"""## b. Train the model using the training data."""

train_df_len = len(train_df)
train=df.iloc[:train_df_len]
test=df.iloc[train_df_len:]

train.head()

test.head()

# input split
X = train.drop(columns=['PassengerId', 'Survived'], axis=1)
y = train['Survived']

X.head()

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

#Building Logistic Regression model and fit to train data

model = LogisticRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_test)

y_pred

"""# **4. Model Evaluation:**

## a. Evaluate the performance of the model on the testing data using accuracy, precision, recall, F1-score, and ROC-AUC score.
"""

#Accuracy
model_accuracy= model.score(x_test, y_test)
model_accuracy

#For Precision,Recall,F-1 score
from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print("classification_report:\n",report)

#For ROC-AUC Curve

print('roc_score:',roc_auc_score(y_test,y_pred))

print('roc_curve:\n',roc_curve(y_test,y_pred))

print('confusion_matrix:\n',confusion_matrix(y_test,y_pred))

"""## Visualize the ROC curve."""

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba (x_test)[:,1])

auc = roc_auc_score(y_test, y_pred)

import matplotlib.pyplot as plt
plt.plot(fpr, tpr, color='red', label='logit model ( area  = %0.2f)'%auc)
plt.plot([0, 1], [0, 1], 'b--') # k-- give diagnoal line --- , b is colour blue colour
plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
plt.ylabel('True Positive Rate')

auc

plt.plot(roc_curve(y_test,y_pred)[0],roc_curve(y_test,y_pred)[1])

"""The plot visually represents the trade-off between correctly classifying positive instances (TPR) and incorrectly classifying negative instances (FPR) for your classification model, along with a summary of its overall performance using the AUC score.

ROC Curve: It plots the trade-off between the true positive rate (TPR) and the false positive rate (FPR) for different threshold values.

fpr: False positive rate. tpr: True positive rate. thresholds: Threshold values used to compute fpr and tpr.

AUC (Area Under the Curve): It provides a single number to summarize the performance of the model, where:

1.0 = perfect classifier.

0.5 = random classifier (represented by the dashed diagonal line).

This plot and AUC score will help assess the classification model’s discriminative ability.

# **5. Interpretation:**

### a. Interpret the coefficients of the logistic regression model.
"""

coefficients = model.coef_[0]
feature_names = X.columns

# Print the coefficients for each feature
for feature, coef in zip(feature_names, coefficients):
  print(f"{feature}: {coef}")

"""1.A positive coefficient means an increase in the feature raises the probability of the positive class (e.g., attorney involvement).  
2.A negative coefficient indicates an increase in the feature lowers the probability of the positive class.  
3.The magnitude of the coefficient reflects the strength of the feature's influence, and exponentiating it gives the odds ratio, showing the impact on odds for each unit change.

### b. Discuss the significance of features in predicting the target variable (survival probability in this case).

In analyzing the Titanic dataset for survival prediction using logistic regression, several features stand out in determining a passenger's likelihood of survival:

Sex (Gender): This is one of the most important features. Female passengers had a significantly higher probability of survival compared to males. In historical context, women and children were prioritized during evacuation.

Passenger Class (Pclass): Higher-class passengers (1st class) had a better chance of survival compared to those in lower classes. This could be due to easier access to lifeboats and better accommodation location on the ship.

Age: Younger passengers, especially children, were more likely to survive. The "women and children first" policy likely influenced this, making age a crucial factor in survival probability.

Fare: Passengers who paid higher fares (which usually correlates with 1st class tickets) had better chances of survival, further reinforcing the importance of socio-economic status.

Family Size: Features like the number of siblings/spouses (SibSp) and parents/children (Parch) aboard influenced survival. Larger family groups often meant a higher survival chance, likely due to familial support in an emergency.

These features, when analyzed through logistic regression coefficients, provide insights into the direction and strength of their impact on survival. For example, a positive coefficient for Sex (female) means that being female increases the odds of survival, while a negative coefficient for Pclass (3rd class) means lower-class passengers are less likely to survive.

# **6. Deployment with Streamlit:**

# Interview Questions:  
###**1.What is the difference between precision and recall?**

Precision is the ratio of true positive predictions to the total predicted positives (true positives + false positives). It measures how many of the predicted positives are actually correct.

Recall, also known as sensitivity, is the ratio of true positives to all actual positives (true positives + false negatives). It measures how many of the actual positives are correctly identified by the model.

Precision is more focused on minimizing false positives, while recall emphasizes minimizing false negatives.

A high precision model is trustworthy when it predicts a positive class, but it may miss some positive instances.

A high recall model catches most of the positive instances but may incorrectly classify some negative cases as positive.

In binary classification, balancing precision and recall is important depending on the problem context (e.g., fraud detection vs. medical diagnosis).

###**2.What is cross-validation, and why is it important in binary classification?**

Cross-validation is a technique for evaluating a model by splitting the dataset into multiple subsets or "folds." The model is trained on some folds and tested on the remaining ones, cycling through each fold.

It helps ensure that the model’s performance is not overly dependent on any specific train-test split.

In binary classification, it reduces the risk of overfitting by giving a more robust estimate of the model's generalization to unseen data.

Common methods include k-fold cross-validation, where the dataset is split into k parts, and the model is validated k times, each time with a different test set.

This process provides a more reliable assessment of how the model performs across varying data subsets.

Cross-validation is crucial in smaller datasets or imbalanced classes to ensure a fair and balanced model evaluation.
"""

from sklearn.linear_model import LogisticRegression
from joblib import dump

# Assuming 'model' is your trained LogisticRegression

dump(model, "logistic_model.joblib")

pip install streamlit

